{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model(x) 前用 model.train() 和 model.eval() 切换网络状态\n",
    "# 不需要计算梯度的代码块用 with torch.no_grad() 包含起来\n",
    "# model.eval() 和 torch.no_grad() 的区别在于\n",
    "# model.eval() 是将网络切换为测试状态，例如 BN 和dropout在训练和测试阶段使用不同的计算方法\n",
    "# torch.no_grad() 是关闭 PyTorch 张量的自动求导机制，以减少存储使用和加速计算，得到的结果无法进行 loss.backward()\n",
    "# model.zero_grad()会把整个模型的参数的梯度都归零, 而optimizer.zero_grad()只会把传入其中的参数的梯度归零\n",
    "# loss.backward() 前用 optimizer.zero_grad() 清除累积梯度\n",
    "\n",
    "import torch.optim as optim\n",
    "# scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=self.lr_milestones, gamma=0.1)\n",
    "scheduler.step()\n",
    "float(scheduler.get_lr()[0])\n",
    "\n",
    "# /home/yaoqf/miniconda3/envs/cvdd/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
    "#   \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
    "\n",
    "# /home/yaoqf/miniconda3/envs/cvdd/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:417: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
    "#   \"please use `get_last_lr()`.\", UserWarning)\n",
    "\n",
    "# torch.optim.lr_scheduler.StepLR(optimizer, optimizer_params[\"lr_decay_step\"], gamma=optimizer_params[\"lr_decay_factor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.train() 和 model.eval() 一般在模型训练和评价的时候会加上这两句，主要是针对由于model在训练时和评价时 Batch Normalization 和 Dropout 方法模式不同；因此，在使用PyTorch进行训练和测试时一定注意要把实例化的model指定train/eval\n",
    "# eval可匹配with torch.no_grad():使用，两者有不同的目标，后者可减少内存使用并加速计算，影响autograd engine并deactivate it，不能backprop\n",
    "## 训练模式允许dropout，反之不允许\n",
    "\n",
    "## model的一些方法：.modules() .train() .eval()\n",
    "\n",
    "## model的一些属性：training/bool、_parameters/OrderedDict()、_buffers/OrderedDict()、_modules/OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim  # 优化器：计算导数的算法\n",
    "# optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1, verbose=False)\n",
    "# 当训练epoch达到milestones值时,初始学习率乘以gamma得到新的学习率\n",
    "# scheduler.step()\n",
    "# optimizer.zero_grad() # Zero the network parameter gradients\n",
    "# forward + backward(loss.backward()) + optimize(optimizer.step())\n",
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=1e-6)\n",
    "## torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "## optim.Adadelta | optim.SGD\n",
    "# Only leaf tensors can be optimised. A leaf tensor is a tensor that was created at the beginning of a graph, i.e. there is no operation tracked in the graph to produce it. In other words, when you apply any operation to a tensor with requires_grad=True it keeps track of these operations to do the back propagation later. You cannot give one of these intermediate results to the optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算模型整体参数量\n",
    "# num_parameters = sum(torch.numel(parameter) for parameter in model.parameters())\n",
    "# sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "# sum([p.data.nelement() for p in model.parameters()])\n",
    "\n",
    "# 可训练参数\n",
    "# net_parameters = filter(lambda p: p.requires_grad, parameters())\n",
    "# params = sum([np.prod(p.size()) for p in net_parameters])\n",
    "\n",
    "# for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
    "# total_params += np.prod(x.data.numpy().shape)\n",
    "\n",
    "# print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size()) > 1, net.parameters()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可以通过model.state_dict()或者model.named_parameters()函数查看现在的全部可训练参数（包括通过继承得到的父类中的参数）\n",
    "# params = list(model.named_parameters())\n",
    "# (name, param) = params[28]\n",
    "# print(name)\n",
    "# print(param.grad)\n",
    "# print('-------------------------------------------------')\n",
    "# (name2, param2) = params[29]\n",
    "# print(name2)\n",
    "# print(param2.grad)\n",
    "# print('----------------------------------------------------')\n",
    "# (name1, param1) = params[30]\n",
    "# print(name1)\n",
    "# print(param1.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型可视化（使用pytorchviz）\n",
    "# szagoruyko/pytorchvizgithub.com\n",
    "# PyTorch可以使用tensorboard来可视化训练过程\n",
    "\t# 运行tensorboard: tensorboard --logdir=runs\n",
    "\t# 使用SummaryWriter类来收集和可视化相应的数据，放了方便查看，可以使用不同的文件夹，比如'Loss/train'和'Loss/test'。\n",
    "\t# from torch.utils.tensorboard import SummaryWriter\n",
    "\t# import numpy as np\n",
    "\t# writer = SummaryWriter()\n",
    "\t# for n_iter in range(100):\n",
    "\t\t# writer.add_scalar('Loss/train', np.random.random(), n_iter)\n",
    "\t\t# writer.add_scalar('Loss/test', np.random.random(), n_iter)\n",
    "\t\t# writer.add_scalar('Accuracy/train', np.random.random(), n_iter)\n",
    "\t\t# writer.add_scalar('Accuracy/test', np.random.random(), n_iter)\n",
    "\n",
    "# 类似 Keras 的 model.summary() 输出模型信息，使用pytorch-summary\n",
    "# sksq96/pytorch-summarygithub.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.manual_seed(args.seed)\n",
    "# torch.cuda.manual_seed(args.seed)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# random.seed(0)\n",
    "# np.random.seed(0)\n",
    "# torch.manual_seed(0)\n",
    "\n",
    "# torch.cuda.manual_seed_all(0)\n",
    "# torch.backends.cudnn.deterministic = True # 打开CUDNN deterministic设置, 会降低训练速度; 且从checkpoints重新开始时会出现意外的结果\n",
    "# torch.backends.cudnn.benchmark = False # 设置为 True，就可以大大提升卷积神经网络的运行速度.可在网络训练开始前设置\n",
    "\n",
    "torch.cuda.set_device(args.gpu)\n",
    "model = model.cuda()\n",
    ".cuda(args.gpu)\n",
    "# 或\n",
    "device = torch.device(\"cuda\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset # 函数参数(dataset, indices): 获取指定一个索引序列对应的子数据集   一个对象 'Subset' object\n",
    "# torch.utils.data.TensorDataset(data_tensor, target_tensor) # 包装数据和目标张量的数据集\n",
    "# torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None, num_workers=0, collate_fn=<function default_collate>, pin_memory=False, drop_last=False) # 其中参数dataset就可使用上面包装的数据集，参数shuffle在训练集中设为真；参数drop_last(如果数据集大小不能被batch size整除，则设置为True后可删除最后一个不完整的batch。如果设为False并且数据集的大小不能被batch size整除，则最后一个batch将更小)\n",
    "## num_workers的经验设置值是自己电脑/服务器的CPU核心数，如果CPU很强、RAM也很充足，就可以设置得更大些\n",
    "## len(loader.dataset)  所有的数据点的个数\n",
    "## DataLoader(dataset, batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False) --> batch_sampler可来自BucketBatchSampler\n",
    "## 训练使用的数据集，开启drop_last，这样如果最后一个batch小于你的batch_size，会扔掉，训练会更稳定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失Loss为Nan或者超级大的原因\n",
    "## 训练或者预测过程中经常会遇到训练损失值或者验证损失值不正常、无穷大、或者直接nan的情况\n",
    "## 可能原因：\n",
    "##      1、梯度爆炸造成Loss爆炸   -->   降低初始的学习率，并设置学习率衰减\n",
    "##      2、损失函数可能不正确   -->   考虑在除数中加入微小的常数保证计算稳定性\n",
    "##      3、batchNorm可能捣鬼  这种情况很有可能发生在预测阶段 即使用model.eval()(Pytorch)之后   -->   如果你在预测阶段也将模型model设置为model.train(True)，那么问题可能就不会出现   或者设置Batchnorm中的参数track_running_stats=False使移动均值和移动方差不起作用\n",
    "##      4、你的Shuffle设置有没有乱动 一般我们是在训练阶段开启shuffle而在预测阶段关闭shuffle   但是假如我们使用了batch_norm层，并且数据的分布极不规律(使用shuflle和不使用shuffle读取的数据顺序的信息分布完全不同)，那么在训练阶段训练好的模型(使用shuffle)，在预测阶段使用的时候(不使用shuffle)，由于数据分布的不同，也是有可能导致batch_norm层出现nan，从而导致不正常的损失函数出现\n",
    "\n",
    "\n",
    "## 调小batch_size，运行速度慢\n",
    "### 观察指标：GPU利用率 显卡内存   -->  数据处理效率可以提高，GPU利用率低了   ->   dataloader可以设置num_worker 一般设置为cpu核的数量   开启pin_memory  .cuda(non_blocking=True)\n",
    "\n",
    "##  加速模型运行\n",
    "### 调大batch_size以及学习率\n",
    "###       增加GPU的内存占用率，尽量用完内存，而不要剩一半，空的内存给另外的程序用，两个任务的效率都会非常低\n",
    "### 设置dataloader的num_worker(4或8或16)和pin_memory 以及non_blocking=True\n",
    "###       但是有时候即使增大num_worker也无法提高GPU利用率，这是因为训练的瓶颈在IO\n",
    "###       num_worker经验值为cpu核心数或gpu的数量\n",
    "### 数据预处理尽量不要放在loader中做\n",
    "\n",
    "\n",
    "# 如何训练能使模型较快收敛: 增大学习率、使用adam这类优化器\n",
    "# 损失为nan或inf： 注意除以0和log0的情况   log(x+1e-6)\n",
    "# 输入不要引入nan  数据要normalized\n",
    "\n",
    "\n",
    "# RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [346, 64, 300]] is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).   -->  x /= ... 改成 x = x / ...\n",
    "\n",
    "\n",
    "\n",
    "# RuntimeError: Caught RuntimeError in replica 0 on device 0. /RuntimeError: CUDA error: device-side assert triggered     使用Pytorch的torch.nn.DataParallel 进行多GPU训练时遇到的一个bug\n",
    "## 下面来具体讲讲nn.DataParallel中是怎么做的。\n",
    "## 首先在前向过程中，你的输入数据会被划分成多个子部分（以下称为副本）送到不同的device中进行计算，而你的模型module是在每个device上进行复制一份，也就是说，输入的batch是会被平均分到每个device中去，但是你的模型module是要拷贝到每个devide中去的，每个模型module只需要处理每个副本即可，当然你要保证你的batch size大于你的gpu个数。然后在反向传播过程中，每个副本的梯度被累加到原始模块中。概括来说就是：DataParallel 会自动帮我们将数据切分 load 到相应 GPU，将模型复制到相应 GPU，进行正向传播计算梯度并汇总\n",
    "## torch.nn.DataParallel(module, device_ids=None, output_device=None, dim=0)  dim只能取值-1或0\n",
    "### 通过在批维度中分块在指定的设备上拆分输入（其他对象将在每个设备上复制一次）\n",
    "\n",
    "# 当出现cuda runtine error  使用命令CUDA_LAUNCH_BLOCKING=1 python your_script.py来追踪\n",
    "\n",
    "\n",
    "# RuntimeError: cuDNN error: CUDNN_STATUS_NOT_INITIALIZED   在cpu上运行   IndexError: index out of range in self\n",
    "\n",
    "\n",
    "\n",
    "## Segmentation fault (core dumped)  \n",
    "### 数据取出来:.cpu().data.numpy()/.data.cpu().numpy()\n",
    "\n",
    "\n",
    "## 基于gpu的提速：链接(https://mp.weixin.qq.com/s/XOgyqeHJJSdDEOYVLABDcg；https://mp.weixin.qq.com/s/Y43Gp82oAeC5LzsF-aa1mg)\n",
    "# 1、选择合适的学习率时间表：选择的学习率时间表对收敛速度以及模型的泛化性能有很大影响；定期增加学习率有助于更快地穿越损失函数中的鞍点\n",
    "# Pytorch 已经实现了这两种方法：「torch.optim.lr_scheduler.CyclicLR」和「torch.optim.lr_scheduler.OneCycleLR」\n",
    "# 2、在DataLoader中使用多个工作程序并固定内存：使用时torch.utils.data.DataLoader，请设置num_workers > 0，而不是默认值0，和pin_memory=True，而不是默认值False；在选择worker数量时，建议将设置为可用GPU数量的四倍；worker数量的多和少都会导致速度变慢，数量越多还会增加CPU内存消耗\n",
    "    # 对于NLP数据，请查看TorchText\n",
    "# 3、批量最大化：在通常情况下，使用GPU内存允许的最大批处理量可以加快训练速度；一般来说，将批量大小增加一倍，学习率也提高一倍\n",
    "# 使用大 batch 的不足是，这可能导致解决方案的泛化能力比使用小 batch 的差\n",
    "# 4、使用自动混合精度（AMP）\n",
    "# 5、使用不同的优化器：比如AdamW，AdamW是带有权重衰减（而不是L2正则化）的Adam，它在错误实现、训练时间都胜过Adam；此外，还有一些非本地的优化器值得关注，比如，LARS和LAMB\n",
    "# 6、打开cudNN基准测试：如果你的模型架构保持固定，输入大小保持不变，则可以设置torch.backends.cudnn.benchmark = True，启动 cudNN 自动调整器，它将对cudNN中计算卷积的多种不同方法进行基准测试，以获得最佳的性能指标\n",
    "# 7、防止CPU和GPU之间频繁传输数据：注意要经常使用tensor.cpu()将tensors从GPU传输到CPU，.item()和.numpy()也是如此，使用.detach()代替；如果正在创建一个张量，就可以使用关键字参数device=torch.device(‘cuda:0’)直接将其分配给你的GPU；如果到传输数据的情境下，可以使用.to(non_blocking=True)，只要你在传输后没有任何同步点\n",
    "# 8、使用梯度/激活检查点checkpointing：检查点的工作原理，是用计算换取内存。\n",
    "# 检查点部分不是讲整个计算图的所有中间激活都存储起来向后计算，而不是保存中间激活，在后传中重新计算。它可以应用到模型的任何部分。具体来说，在前向传递中，函数将以torch.no_grad()的方式运行，即不存储中间的激活。相反，前向传递会保存输入元组和函数参数。在后向传递中，检索保存的输入和函数，然后再次对函数进行前向传递计算，现在跟踪中间激活，使用这些激活值计算梯度。虽然这可能会略微增加你在给定批量大小下的运行时间，但你会显著减少你的内存占用。这反过来又会让你进一步增加你所使用的批次大小，提高GPU的利用率\n",
    "# 9、使用梯度累积：另一种增加批次大小的方法是在调用optimizer.step()之前，在多个.backward()通道中累积梯度；这个方法主要是为了规避GPU内存限制而开发的，但不清楚是否有额外的.backward()循环之间的权衡\n",
    "# 10、使用DistributedDataParallel进行多GPU训练：使用 torch.nn.DistributedDataParallel 而不是 torch.nn.DataParallel；这样做可以让每个GPU将由一个专门的CPU核驱动，避免了DataParallel的GIL问题\n",
    "# 11、将梯度设置为None而不是0：使用.zero_grad(set_to_none=True)而不是.zero_grad()；这样做会让内存分配器来处理梯度，而不是主动将它们设置为0，这样会适度加速；注意，这样做并不是没有副作用的\n",
    "# 12、使用 .as_tensor 而不是 .tensor()：torch.tensor() 总是复制数据。如果你有一个要转换的 numpy 数组，使用 torch.as_tensor() 或 torch.from_numpy() 来避免复制数据\n",
    "# 13、如果不需要，请关闭调试API：Pytorch提供了很多调试工具，例如autograd.profiler，autograd.grad_check和autograd.anomaly_detection，确保在需要的时候使用它们，不需要时将其关闭，否则他们会拖慢你的训练速度\n",
    "# 14、使用梯度剪裁：剪裁梯度，可以加速加速收敛。最初是用来避免RNNs中的梯度爆炸，可以使用orch.nn.utils.clipgrad_norm来实现；目前尚不清楚哪些模型能靠梯度剪裁能够加速多少，但它似乎对RNNs、基于 Transformer 和 ResNets 的架构以及一系列不同的优化器都非常有用\n",
    "# 15、在BatchNorm之前关闭偏置：在BatchNormalization图层之前关闭图层的偏置；对于二维卷积层，可以通过将bias关键字设置为False：来完成torch.nn.Conv2d(…, bias=False, …)\n",
    "# 16、在验证过程中关闭梯度计算：在验证期间设置torch.no_grad()\n",
    "# 17、使用输入和批次归一化\n",
    "# 18、数据变换 (用于数据增强) 可成为速度提升的另一个来源。一些只使用简单 Python 语句的变换可以通过使用 numba 包来加速\n",
    "# 19、将数据集预处理成单个文件，对速度也有好处\n",
    "\n",
    "\n",
    "# 训练神经网络的tips和tricks\n",
    "# 1、查阅pytorch文档: https://pytorch.org/docs/\n",
    "# 2、分析你的数据\n",
    "    # 是否需要预处理：神经网络在它们的输入和输出都具有相同幅度the same order of magnitude时学习最好，如果数据不符合，则需要rescale和offset\n",
    "    # 是否具有对称性：如果数据集具有对称性，则可用于设计模型。例如图像常具有translation对称性，在这种情况下利用卷积层是个好主意。对称性也可用于人工增加数据集大小(通过data augmentation)\n",
    "# 3、考虑你的网络架构：包括层数、层的类型和宽度、激活函数等\n",
    "    # 使用小的网络开始，然后逐渐增加复杂度\n",
    "    # 宽度和深度同样重要\n",
    "    # 特殊层如卷积或LSTM需要利用数据的某种对称性(如空间或时间)\n",
    "    # 考虑梯度流flow: 网络的所有部分是否至少有one path to the loss(在计算图中)(梯度将flow without vanishing)\n",
    "        # highly saturating激活函数如sigmoids或hyperbolic tangent不应该被堆叠太多，因为它们会squish梯度\n",
    "        # ResNets connect directly the output of each residual block to the loss\n",
    "# 4、好的训练实践\n",
    "    # 使用验证集：确保不过拟合\n",
    "    # 保留测试集：模型的最后评估\n",
    "    # 在训练时shuffle数据集：对数据集中元素的顺序不过拟合\n",
    "    # 一次更改一个东西，并追踪你所做的改变\n",
    "    # 从一个小数据集开始：在进行一个长时间的训练之前，先以数据集的一个小子集开始，来确保能正确运行，这能避免浪费时间(当训练要结束时由于一个小的错误而失败)\n",
    "# 5、改善一个不成功的网络：\n",
    "    # 5.1: pytorch的一般错误\n",
    "        # 确保设置好优化器，如初始化时传入model.parameters()，每次迭代时调用optimizer.step()\n",
    "        # 不要忘记zero梯度: 要在调用.backward()(on your loss)之前调用.zero_grad()(on your model)。否则当前迭代的梯度会叠加上之前的梯度，使得模型不被正确训练\n",
    "        # 不要使用太大的minibatches: 太大的minibatches会用完计算机内存，特别当训练的模型很大时。会极大使训练降速甚至崩溃\n",
    "        # 不要accumulate变量: 如果需要存储网络的输出值或损失值，确保调用.item()(for numbers)或.detach()(for tensors)，并使用它们的返回值。否则会阻止pytorch在每次迭代时从内存释放计算图，并很快填满计算机RAM，极大降速\n",
    "    # 5.2: 模型似乎没学到任何东西\n",
    "        # 检查学习率：学习率太低的优化器会使得你的模型以一个非常慢的速度学习，但太大的学习率会使得模型不稳定。最优学习率取决于网络结构和minibatch size\n",
    "        # 检查损失计算：如符号是否错误\n",
    "        # 模型是否被正确初始化\n",
    "        # 检查数值稳定性：确保你的损失或网络输出不是nan或inf(可能是学习率太大、初始化不当、引入一些数值不稳定计算)。尽量避免torch.log(F.sigmoid(x))，因为sigmoid会saturate to 0，从而引起log返回inf\n",
    "        # 增大网络能力：可能你的网络不够有力来学习你的任务，可尝试添加层或增加隐含层的宽度\n",
    "    # 5.3: 网络过拟合(典型表现即训练性能比验证性能好得多)\n",
    "        # 简化网络：减少层数直到它不再过拟合，然后再慢慢重新增加复杂度\n",
    "        # 添加一些正则化：正则化方法如添加一个网络权重的惩罚或者优化器的权重衰减或者更复杂的方法如dropout或batch normalization，可帮助网络的泛化。但它们也会极大地损害整体的性能，所以要小心，有时最好不要用\n",
    "        # 增强augment数据：如果你的数据集具有对称性(可被用于数据增强)，增强数据可极大帮助泛化\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
