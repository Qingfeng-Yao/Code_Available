{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.nn.CrossEntropyLoss 的输入不需要经过 Softmax\n",
    "# torch.nn.CrossEntropyLoss 等价于 torch.nn.functional.log_softmax + torch.nn.NLLLoss\n",
    "\n",
    "# F.cross_entropy(input, target, reduction='none', weight=weight) # weight确定每个类别的权重\n",
    "\t# 该函数使用了 log_softmax 和 nll_loss\n",
    "# F.nll_entropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = nn.CosineSimilarity(dim=2)\n",
    "cosine_sim(M, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "seq = torch.tensor([[1,2,0], [3,0,0], [4,5,6]])\n",
    "lens = [2, 1, 3]\n",
    "packed = pack_padded_sequence(seq, lens, batch_first=True, enforce_sorted=False) # `lengths` array must be sorted in decreasing order when `enforce_sorted` is True\n",
    "'''\n",
    "PackedSequence(data=tensor([4, 1, 3, 5, 2, 6]), batch_sizes=tensor([3, 2, 1]), sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0]))\n",
    "'''\n",
    "seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch_first=True)\n",
    "'''\n",
    "(tensor([[1, 2, 0],\n",
    "         [3, 0, 0],\n",
    "         [4, 5, 6]]), tensor([2, 1, 3]))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Embedding(2, 5)，这里的2表示有2个词，5表示5维度，其实也就是一个2x5的矩阵  .weight可学习权重，从标准正态分布初始化/2x5\n",
    "# nn.Parameter: Variable的一种 参数为data\n",
    "# nn.CosineSimilarity(dim=2)\n",
    "# nn.Linear(20, 30)  # 20,30是指维度  .weight的shape为30,20\n",
    "# nn.Sequential(*layers)  A sequential container  Modules will be added to it in the order they are passed in the constructor   |   layers = [module1, module2, ...]   \n",
    "# nn.Dropout(dropout)\n",
    "# 激活函数 nn.ReLU | nn.Tanh() | nn.ELU\n",
    "## 'relu': Rectified Linear Unit整流线性单位函数/修正线性单元，通常指代以斜坡函数f(x)=max(0,x)及其变种为代表的非线性函数  -->  nn.ReLU\n",
    "## 双曲函数：'tanh'、逻辑函数/Logistic sigmoid  -->  nn.Tanh  | nn.Sigmoiid\n",
    "# nn.LSTM(input_size, hidden_size, num_layers, bias/True, batch_first/False, dropout, bidirectional/False)   batch_first --> (batch, seq, feature)\n",
    "## RuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED \n",
    "##           --> torch版本和cuda版本不一致或者是h0, c0 are not moved to GPU in your model.   h0, c0 = h0.cuda(), c0.cuda()\n",
    "## UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters()’\n",
    "##           --->  使用flatten_parameters()把权重存成连续的形式，可以提高内存利用率。具体的使用方法就是在所使用的RNN模块的forward函数下加上self.rnn.flatten_parameters()  #self.rnn是我所使用的RNN\n",
    "## `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "# 神经网络类继承nn.Module  除__init__函数，还要实现forward函数(从输入到输出)\n",
    "## self.register_buffer(name: str, tensor: torch.Tensor, persistent: bool = True) → None  Adds a buffer to the module\n",
    "## register_parameter(name: str, param: torch.nn.parameter.Parameter) → None   Adds a parameter to the module. \n",
    "## module.weight.data | module.bias.data\n",
    "# nn.init.orthogonal_(tensor, gain=1) 使得tensor是正交的\n",
    "## nn.init.xavier_uniform_()\n",
    "# nn.CrossEntropyLoss(weight, reduction: str = 'mean')  参数weight:为每个类指定权重  |  reduction: 对输出应用，'none' | 'mean' | 'sum' \n",
    "# nn.utils.rnn.pad_packed_sequence(sequence, batch_first=False, padding_value=0.0, total_length=None)  填充一批包装好的可变长度序列  The returned Tensor’s data will be of size T x B x *, where T is the length of the longest sequence and B is the batch size. If batch_first is True, the data will be transposed into B x T x * format.\n",
    "## nn.utils.rnn.pack_padded_sequence(input, lengths, batch_first=False, enforce_sorted=True) : 压缩包含可变长度填充序列的张量   input can be of size T x B x * where T is the length of the longest sequence (equal to lengths[0]), B is the batch size, and * is any number of dimensions (including 0). If batch_first is True, B x T x * input is expected.   |    For unsorted sequences, use enforce_sorted = False. If enforce_sorted is True, the sequences should be sorted by length in a decreasing order, i.e. input[:,0] should be the longest sequence, and input[:,B-1] the shortest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "# F.softmax(input, dim=1)  # 按行softmax,行和为1"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
