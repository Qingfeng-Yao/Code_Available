{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert import BertTokenizer\n",
    "# class MyBertTokenizer(BertTokenizer): # 自定义Tokenizer\n",
    "# 父类有属性: .vocab .tokenize(text)(其中text为空格隔开的字符串) .from_pretrained('bert-base-uncased', cache_dir=root)\n",
    "\n",
    "# def __init__(self, vocab_file, do_lower_case=True, max_len=None, append_eos=False):\n",
    "#         super().__init__(vocab_file, do_lower_case=do_lower_case, max_len=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.modeling import BertModel\n",
    "bert = BertModel.from_pretrained(pretrained_model_name_or_path=pretrained_model_name, cache_dir=cache_dir) # assert pretrained_model_name in ('bert-base-uncased', 'bert-large-uncased', 'bert-base-cased')\n",
    "embedding = bert.embeddings\n",
    "embedding_size = embedding.word_embeddings.embedding_dim\n",
    "# Remove BERT model parameters from optimization\n",
    "for param in bert.parameters():\n",
    "\tparam.requires_grad = False\n",
    "bert.eval()  # make sure bert is in eval() mode\n",
    "hidden, _ = bert(x.transpose(0, 1), output_all_encoded_layers=False)  # output only last layer\n",
    "# x.shape = (sentence_length, batch_size)\n",
    "# hidden.shape = (batch_size, sentence_length, hidden_size)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
