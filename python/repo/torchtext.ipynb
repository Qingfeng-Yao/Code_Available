{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.data as data\n",
    "from torchtext.vocab import Vectors\n",
    "\n",
    "torchtext.data.Field()\n",
    "# lower: 是否把数据转化为小写 默认值: False.\n",
    "# sequential: 是否把数据表示成序列，如果是False, 不能使用分词 默认值: True.\n",
    "# batch_first\n",
    "\n",
    "regex = re.compile(r'[^\\u4e00-\\u9fa5aA-Za-z0-9]')\n",
    "def word_cut(text):\n",
    "    text = regex.sub(' ', text)\n",
    "    return [word for word in jieba.cut(text) if word.strip()]\n",
    "\n",
    "text_field = data.Field(lower=True)\n",
    "label_field = data.Field(sequential=False)\n",
    "text_field.tokenize = word_cut\n",
    "train, dev = data.TabularDataset.splits(\n",
    "        path='data', format='tsv', skip_header=True,\n",
    "        train='train.tsv', validation='dev.tsv', test='',\n",
    "        fields=[\n",
    "            ('index', None),\n",
    "            ('label', label_field),\n",
    "            ('text', text_field)\n",
    "        ]\n",
    "    )\n",
    "def load_word_vectors(model_name, model_path):\n",
    "    vectors = Vectors(name=model_name, cache=model_path)\n",
    "    return vectors\n",
    "vectors = load_word_vectors(args.pretrained_name, args.pretrained_path)\n",
    "text_field.build_vocab(train_dataset, dev_dataset, vectors=vectors)或text_field.build_vocab(train_dataset, dev_dataset)\n",
    "label_field.build_vocab(train_dataset, dev_dataset)\n",
    "# build_vocab还有参数max_size\n",
    "\n",
    "train_iter, dev_iter = data.Iterator.splits(\n",
    "        (train_dataset, dev_dataset),\n",
    "        batch_sizes=(args.batch_size, len(dev_dataset)),\n",
    "        sort_key=lambda x: len(x.text),\n",
    "        device=args.device, repeat=False, shuffle=True)\n",
    "\n",
    "# for batch in train_iter: batch.text, batch.label, batch.batch_size\n",
    "# len(data_iter.dataset)\n",
    "\n",
    "args.vocabulary_size = len(text_field.vocab)\n",
    "embedding_dim = text_field.vocab.vectors.size()[-1]\n",
    "class_num = len(label_field.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2, WikiText103\n",
    "WikiText103.splits(root=root, text_field=TEXT:data.Field)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
