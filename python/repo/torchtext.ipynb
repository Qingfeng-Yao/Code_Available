{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.data as data # 使用torchtext加载文本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.Field(sequential=True, tokenize=tokenize, include_lengths=True) / lower=True\n",
    "\t# tokenize = lambda x: x.split()或者def word_cut(text):text = regex.sub(' ', text) return [word for word in jieba.cut(text) if word.strip()], 其中regex = re.compile(r'[^\\u4e00-\\u9fa5aA-Za-z0-9]')\n",
    "\t\t# text_field.tokenize = word_cut\n",
    "\t\t# tokenize: 分词函数, 默认为str.split\n",
    "\t\t# nltk.word_tokenize\n",
    "\t\t# data.Field(tokenize='spacy', tokenizer_language='en_core_web_md')\n",
    "\t# include_lengths设为True是为了方便之后使用torch的pack_padded_sequence\n",
    "\t# lower: 是否把数据转化为小写 默认值: False. \n",
    "\t# sequential: 是否把数据表示成序列，如果是False, 不能使用分词 默认值: True.\n",
    "\t# batch_first: batch作为第一个维度\n",
    "\t# use_vocab: 是否使用词典, 默认为True; 如果为False，那么输入的数据类型必须是数值类型(即使用vocab转换后的)\n",
    "\t# init_token: 文本的初始字符, 默认为None\n",
    "\t# eos_token: 文本的结束字符, 默认为None\n",
    "\t# pad_token: 用于补全的字符, 默认为<pad>\n",
    "\t# unk_token: 替换袋外词的字符，默认为<unk>\n",
    "\t# tensor_type: 把数据转换成的tensor类型 默认值为torch.LongTensor\n",
    "\t# fix_length: 所有样本的长度, 不够则使用pad_token补全, 默认为None, 表示灵活长度\n",
    "# 标签也可用data.Field定义, 即data.Field(sequential=False, use_vocab=False)\n",
    "# data.LabelField(sequential=False, use_vocab=False, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, dev, test = data.TabularDataset.splits(\n",
    "#         path='data', format='tsv', skip_header=True,\n",
    "#         train='train.tsv', validation='dev.tsv', test='',\n",
    "#         fields=[\n",
    "#             ('index', None),\n",
    "#             ('label', label_field),\n",
    "#             ('text', text_field)\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "# 可选format='csv'\n",
    "# skip_header: 是否跳过文件的第一行\n",
    "# fields列表存储的Field的顺序必须和csv/tsv文件中每一列的顺序对应, 不需要可添加None\n",
    "# 如果只想加载训练集, 不给validation和test参数赋值\n",
    "\t# data.TabularDataset.splits返回的是一个元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 除了用表格数据加载, 还可以使用Example, 即from torchtext.data import Example\n",
    "# Example.fromlist([text,label],fields=fields)  \n",
    "\t# 其中fields同上为列表, text对应一个字符串文本, label为对应的标签(int)\n",
    "\t# 上述为一个example, 最后得到一个examples列表\n",
    "\t# 每个example可以访问它的fields, 如example.comment(为分词后列表)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchtext.data.example.Example'>\n",
      "[\"d'aww!\", 'he', 'matches', 'this', 'background', 'colour.', 'really', 'good,', 'pleace', 'tell', 'him', 'the', 'good', 'news']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import Field,LabelField,Example,Dataset\n",
    "import torch\n",
    "\n",
    "corpus = [\"D'aww! He matches this background colour. really good, pleace tell him the good news\",\n",
    "         \"Yo bitch Ja Rule is more succesful then\",\n",
    "         \"If you have a look back at the source\",\n",
    "\t\t \"hello, mary\",\n",
    "\t\t \"hi, nice to meet you\"]\n",
    "labels = [2,3,1,2,0]\n",
    "\n",
    "TEXT = Field(sequential=True, lower=True, fix_length=10, tokenize=str.split,batch_first=True)\n",
    "# LABEL = Field(sequential=False, use_vocab=False)\n",
    "LABEL = LabelField(sequential=False, use_vocab=False, dtype=torch.long)\n",
    "fields = [(\"comment\", TEXT),(\"label\",LABEL)]\n",
    "\n",
    "examples = []\n",
    "for text,label in zip(corpus,labels):\n",
    "    example = Example.fromlist([text,label],fields=fields)\n",
    "    examples.append(example)\n",
    "print(type(examples[0]))\n",
    "print(examples[0].comment)\n",
    "print(examples[0].label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载完数据可以开始建词表\n",
    "from torchtext.vocab import Vectors\n",
    "def load_word_vectors(model_name, model_path):\n",
    "    vectors = Vectors(name=model_name, cache=model_path) # 如vectors = Vectors(name='glove.6B.300d.txt/glove.840B.300d', cache='.vector_cache')\n",
    "    return vectors\n",
    "# vectors = load_word_vectors(pretrained_name, pretrained_path)\n",
    "# text_field.build_vocab(train_dataset, dev_dataset, vectors=vectors)\n",
    "    # 或text_field.build_vocab(train_dataset, dev_dataset)\n",
    "    # 建词表一般是用训练集建，不要用验证集和测试集\n",
    "    # max_size: 单词表容量\n",
    "    # unk_init=torch.Tensor.normal_ / 初始化train_data中不存在预训练词向量词表中的单词\n",
    "# label_field.build_vocab(train_dataset, dev_dataset)\n",
    "\n",
    "# vocabulary_size = len(text_field.vocab) / class_num = len(label_field.vocab)\n",
    "    # .vocab.freqs.most_common(20) 数据集里最常出现的20个单词\n",
    "    # .vocab.itos 列表 index to word\n",
    "    # .vocab.stoi 字典 word to index\n",
    "    # pretrained_embeddings = .vocab.vectors\n",
    "        # model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "        # UNK_IDX = .vocab.stoi[field.unk_token]\n",
    "        # PAD_IDX = .vocab.stoi[field.pad_token]\n",
    "        # 因为预训练的权重的unk和pad的词向量不是在我们的数据集语料上训练得到的，所以最好置零\n",
    "        # model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "        # model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "    # embedding_dim = text_field.vocab.vectors.size()[-1]\n",
    "\n",
    "import torchtext\n",
    "torchtext.vocab.GloVe(name='840B', dim=300)\n",
    "torchtext.vocab.FastText(language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10, 14, 23, 35,  7,  9, 29, 12, 28, 33],\n",
      "        [37,  8, 20, 30, 19, 25, 32, 34,  1,  1],\n",
      "        [18,  3, 13,  4, 21,  6,  5,  2, 31,  1],\n",
      "        [15, 22,  1,  1,  1,  1,  1,  1,  1,  1],\n",
      "        [16, 27, 36, 24,  3,  1,  1,  1,  1,  1]])\n",
      "defaultdict(<function _default_unk_index at 0x7f642d82b3b0>, {'<unk>': 0, '<pad>': 1, 'the': 2, 'you': 3, 'a': 4, 'at': 5, 'back': 6, 'background': 7, 'bitch': 8, 'colour.': 9, \"d'aww!\": 10, 'good': 11, 'good,': 12, 'have': 13, 'he': 14, 'hello,': 15, 'hi,': 16, 'him': 17, 'if': 18, 'is': 19, 'ja': 20, 'look': 21, 'mary': 22, 'matches': 23, 'meet': 24, 'more': 25, 'news': 26, 'nice': 27, 'pleace': 28, 'really': 29, 'rule': 30, 'source': 31, 'succesful': 32, 'tell': 33, 'then': 34, 'this': 35, 'to': 36, 'yo': 37})\n",
      "defaultdict(<function _default_unk_index at 0x7f642d82b3b0>, {2: 0, 0: 1, 1: 2, 3: 3})\n",
      "<class 'collections.Counter'>\n",
      "1\n",
      "<pad>\n",
      "0\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "new_corpus = [example.comment for example in examples]\n",
    "new_labels = [example.label for example in examples]\n",
    "TEXT.build_vocab(new_corpus) # 或者new_corpus = Dataset(examples, fields)\n",
    "LABEL.build_vocab(new_labels)\n",
    "print(TEXT.process(new_corpus))\n",
    "print(TEXT.vocab.stoi)\n",
    "print(LABEL.vocab.stoi) # {'<unk>': 0, 2: 1, 0: 2, 1: 3, 3: 4} / {2: 0, 0: 1, 1: 2, 3: 3}\n",
    "\n",
    "print(type(TEXT.vocab.freqs)) # freqs是一个Counter对象, 包含了词表中单词的计数信息\n",
    "print(TEXT.vocab.freqs['at'])\n",
    "print(TEXT.vocab.itos[1]) # itos表示index to str\n",
    "print(TEXT.vocab.stoi['<unk>']) # stoi表示str to index\n",
    "print(TEXT.vocab.vectors) # 词向量\n",
    "# TEXT.vocab.load_vectors('fasttext.en.300d') # 自动下载并加载词向量\n",
    "# print(TEXT.vocab.vectors.shape) torch.Size([25, 300])\n",
    "\n",
    "# p = os.path.expanduser(\"vector_cache/sgns.wiki.bigram-char\") # 加载本地词向量\n",
    "# TEXT.vocab.load_vectors(vocab.Vectors(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用torchtext的迭代器来批量加载数据，torchtext.data里的BucketIterator非常好用\n",
    "\t# data.BucketIterator.splits不但返回的是元组，它的参数datasets要求也是以元组形式\n",
    "# 它可以把长度相近的文本数据尽量都放到一个batch里，这样最大程度地减少padding\n",
    "# 数据就少了很多无意义的0，也减少了矩阵计算量\n",
    "# train_iter, dev_iter = data.Iterator.splits(\n",
    "#         (train_dataset, dev_dataset),\n",
    "#         batch_sizes=(args.batch_size, len(dev_dataset)),\n",
    "#         sort_key=lambda x: len(x.text),\n",
    "#         device=args.device, repeat=False, shuffle=True)\n",
    "# 参数\n",
    "# sort_within_batch设为True的话, 一个batch内的数据就会按sort_key的排列规则降序排列\n",
    "# sort_key是排列的规则\n",
    "# batch_size\n",
    "\n",
    "# for batch in train_iter: batch.text, batch.label, batch.batch_size\n",
    "# len(data_iter.dataset)\n",
    "# 若inclue_lengths为True, 则还会包含一个句子长度的Tensor, 即返回(seq_length, batch_size)和(batch_size, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import WikiText2, WikiText103\n",
    "# WikiText103.splits(root=root, text_field=TEXT) # TEXT/data.Field\n",
    "\n",
    "from torchtext.datasets import IMDB\n",
    "\n",
    "train_data, test_data = IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "print('len of train data:', len(train_data))        # 25000\n",
    "print('len of test data:', len(test_data))          # 25000\n",
    "\n",
    "# torchtext.data.Example : 用来表示一个样本，数据+标签\n",
    "print(train_data.examples[15].text)                 # 文本：句子的单词列表\n",
    "print(train_data.examples[15].label)                # 标签: 积极\n",
    "\n",
    "import random\n",
    "SEED = 1234\n",
    "train_data, valid_data = train_data.split(random_state=random.seed(SEED))\n",
    "\n",
    "TEXT.build_vocab(train_data, max_size=10000, vectors='glove.6B.100d') # unk_init=torch.Tensor.normal_\n",
    "LABEL.build_vocab(train_data)\n",
    "\n",
    "print(len(TEXT.vocab))             # 10002\n",
    "print(TEXT.vocab.itos[:12])        # ['<unk>', '<pad>', 'the', ',', '.', 'and', 'a', 'of', 'to', 'is', 'in', 'I']\n",
    "print(TEXT.vocab.stoi['and'])      # 5\n",
    "print(LABEL.vocab.stoi)            # defaultdict(None, {'neg': 0, 'pos': 1})\n",
    "print(TEXT.vocab.freqs.most_common(20))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94b8ce2ec9a5bb238eaac0e1491a1a39f3b317a5c834f567228f3f456bc35b60"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
