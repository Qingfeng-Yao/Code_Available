{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.utils import datasets_iterator # 用在循环中，对数据进行迭代\n",
    "# ImportError: cannot import name 'datasets_iterator' from 'torchnlp.utils'\n",
    "## Deprecated torchnlp.utils.datasets_iterator in favor of Pythons itertools.chain.\n",
    "\n",
    "from torchnlp.encoders.text import SpacyEncoder # 第一个参数为文本字符串列表 .encode得到字符串的编码整数序列 .vocab得到词汇列表 .decode输入字符串编码序列得到对应的字符串 .vocab_size词汇表大小\n",
    "from torchnlp.encoders.text.default_reserved_tokens import DEFAULT_EOS_INDEX, DEFAULT_UNKNOWN_INDEX\n",
    "from torchnlp.encoders.text.default_reserved_tokens import DEFAULT_PADDING_INDEX\n",
    "# print(DEFAULT_EOS_INDEX, DEFAULT_UNKNOWN_INDEX, DEFAULT_PADDING_INDEX, DEFAULT_SOS_INDEX) # 2 1 0 3 \n",
    "# from torchnlp.encoders.text.text_encoder import stack_and_pad_tensors\n",
    "## 返回SequenceBatch: Padded tensors and original lengths of tensors. # 按一个batch中最长的进行pad，不足的以pad值填充\n",
    "\n",
    "from torchnlp.word_to_vector import GloVe, FastText\n",
    "# GloVe：参数为name，可取['6B', '42B', '840B', 'twitter.27B']；参数cache，设置词向量缓存地址；若name为'6B'或'twitter.27B'，还有一个参数为dim，即确定获取的词向量维数\n",
    "# FastText：参数为language='en', cache同上\n",
    "\n",
    "  \n",
    "# bert\n",
    "# # 使用自定义Tokenizer会报错 \"TypeError: __init__() got an unexpected keyword argument 'max_len'\"  使用BertTokenizer不会报错  需要在自定义Tokenizer的__init__()中添加参数max_len\n",
    "# 同时自定义Tokenizer里添加了特殊字符[UNK]，此时需要和模型BertModel词汇表大小保持一致，否则会出现错误 \"IndexError: index out of range in self\"\n",
    "\n",
    "\n",
    "# torchtext.data.Field()   参数：include_lengths=False/为真即返回一个元组，包括一个padded minibatch和一个含有每个例子长度的list   ｜   unk_token=“<unk>”  |   tokenize=(lambda s: list(s.strip()))/If “spacy”, the SpaCy tokenizer is used/Default: string.split.\n",
    "## .preprocess(string)  /处理单一例子 tokenizing\n",
    "# from torchtext import data\n",
    "## data.Dataset(examples, fields, filter_pred=None)\n",
    "### .splits(path=None, root='.data', train=None, validation=None, test=None, **kwargs)\n",
    "## data.Example.fromlist(data, fields) 定义一个例子\n",
    "## torchtext.data.BucketIterator(dataset, batch_size, sort_key=None, device=None, batch_size_fn=None, train=True, repeat=False, shuffle=None, sort=None, sort_within_batch=None) : batches examples of similar lengths together.\n",
    "### .splits()\n",
    "\n",
    "\n",
    "\n",
    "# from torchnlp.samplers import BucketBatchSampler\n",
    "# BucketBatchSampler(dataset.train_set, batch_size=args.batch_size, drop_last=False, sort_key=lambda r: len(r['text'])) 按长度从大到小排列  然后以批大小重新组合数据\n",
    "\n",
    "\n",
    "\n",
    "# Calling FastText() results to HTTP Error 301: Moved Permanently\n",
    "## from torchnlp.word_to_vector import FastText 出错\n",
    "## from torchtext.vocab import FastText torchtext==0.6.0 可以\n",
    "\n",
    "\n",
    "\n",
    "# from pytorch_pretrained_bert import BertTokenizer\n",
    "##  自定义MyBertTokenizer(BertTokenizer)  MyBertTokenizer.from_pretrained('bert-base-uncased', cache_dir=root)  ：问题TypeError: __init__() got an unexpected keyword argument 'max_len'  -->    在自定义的init里加上max_len=None\n",
    "## 原始BertTokenizer: def __init__(self, vocab_file, do_lower_case=True, max_len=None, do_basic_tokenize=True, never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\")):\n",
    "# from pytorch_pretrained_bert.modeling import BertModel\n",
    "## BertModel.from_pretrained(pretrained_model_name=pretrained_model_name, cache_dir=cache_dir) --> 改为BertModel.from_pretrained(pretrained_model_name_or_path=pretrained_model_name, cache_dir=cache_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
