{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 搭配numpy/scipy使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本编码--TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "token_pattern = \"(?u)\\\\b\\\\w+\\\\b\"\n",
    "tfidf_vectorizer = TfidfVectorizer(token_pattern=token_pattern)\n",
    "# tfidf_feature = tfidf_vectorizer.fit_transform(texts)  # texts是文本列表，每个元素是空格连接的字符串\n",
    "\n",
    "count = CountVectorizer()\n",
    "# tf = count.fit_transform(texts) \n",
    "\t# tf.toarray(): 查看词频输出\n",
    "\t# count.vocabulary_: 查看词汇表\n",
    "transformer = TfidfTransformer()\n",
    "# tfidf = transformer.fit_transform(tf) \n",
    "\t# tfidf = transformer.transform(tf) # 用训练集拟合, 对测试集直接进行变换, 此处的tf为测试集的词频统计\n",
    "# tfidf.toarray().astype(np.float32)\n",
    "# tfidf[i, row['text']].toarray().astype(np.float32).flatten()\n",
    "\t# 输入文本已经被编码的情况下使用这个方式计算tfidf\n",
    "\t# counts_sample = torch.bincount(row['text'])\n",
    "\t# tf[i, :len(counts_sample)] = counts_sample.cpu().data.numpy()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "vectorizer = LabelEncoder()\n",
    "# X = vectorizer.fit_transform(texts) # 为每个文本元素分配一个索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 降维\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# svd = TruncatedSVD(n_components)\n",
    "# normalizer = Normalizer(copy=False)\n",
    "# lsa = make_pipeline(svd, normalizer)\n",
    "# lsa_feature = lsa.fit_transform(X)\n",
    "\n",
    "# 主成分分析法(PCA)\n",
    "from sklearn.decomposition import PCA\n",
    "# PCA(n_components=2).fit_transform(X)\n",
    "# 参数n_components为主成分数目\n",
    "\n",
    "# 线性判别分析法(LDA)\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "# LDA(n_components=2).fit_transform(X)\n",
    "# 参数n_components为降维后的维数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚类\n",
    "from sklearn.cluster import KMeans\n",
    "# kmeans = KMeans(n_clusters).fit(X) # X是嵌入矩阵\n",
    "# centers = kmeans.cluster_centers_ / np.linalg.norm(kmeans.cluster_centers_, ord=2, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指标\n",
    "from sklearn.metrics import confusion_matrix\n",
    "# confusion_matrix(y_true, y_score).astype(float) # 输出一个二维矩阵, 维数与类总数相等, 输入都是numpy\n",
    "# 横轴预测分类, 纵轴真实分类\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "# accuracy_score(y_true, y_pred, normalize=True)\n",
    "\t# normalize: 默认值为True, 返回正确分类的比例; False, 返回正确分类的样本数\n",
    "\t# 在正负样本不平衡的情况下, 准确率这个评价指标有很大的缺陷\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_score, recall_score\n",
    "# roc_auc_score(y_true, y_score, average='macro')\n",
    "\t# ROC曲线下的面积就是AUC的值\n",
    "\t# 二分类时直接使用roc_auc_score(y_true, y_score)\n",
    "\t# 参数average可以取: \n",
    "\t\t# micro(全局指标)\n",
    "\t\t# macro(计算每个标签的指标并计算未加权平均/没有考虑标签不平衡)\n",
    "\t\t# weighted(计算每个标签的指标并计算加权平均/考虑标签不平衡/可能导致F得分不在精确率和召回率之间)\n",
    "# average_precision_score(y_true, y_score, average='macro')\n",
    "\t# 计算平均精度(AP)\n",
    "# f1_score(y_true, y_pred, pos_label=1, average='binary')\n",
    "\t# f1 = 2 * (precision * recall) / (precision + recall) \n",
    "# precision_score(y_true, y_pred, pos_label=1, average='binary')\n",
    "\t# 查准率或者精度； precision=TP/(TP+FP)\n",
    "# recall_score(y_true, y_pred, pos_label=1, average='binary')\n",
    "\t# 查全率: recall=TP/(TP+FN)\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集\n",
    "from sklearn.datasets import load_iris\n",
    "# 导入IRIS数据集\n",
    "iris = load_iris()\n",
    "iris.data # 特征矩阵: array二维数组，包含4个特征\n",
    "iris.target # 目标向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理\n",
    "\t# 无量纲化: 包括标准化, 区间缩放法, 归一化\n",
    "\t# 对定量特征二编码: Binarizer\n",
    "\t# 对定性特征哑编码: OneHotEncoder\n",
    "\t# 缺失值计算: Imputer\n",
    "\t# 数据变换: PolynomialFeatures, FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# 标准化: 需要计算特征的均值和标准差(按照列进行)\n",
    "StandardScaler().fit_transform(iris.data) # 返回值为标准化后的数据\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# 区间缩放法: 思路有多种，常见的一种为利用两个最值进行缩放\n",
    "MinMaxScaler().fit_transform(iris.data) # 返回值为缩放到[0, 1]区间的数据\n",
    "\n",
    "from sklearn.preprocessing import Normalizer\n",
    "# 归一化: 将每个样本缩放到单位范数, 即每个样本的范数为1\n",
    "Normalizer().fit_transform(iris.data) # 返回值为归一化后的数据\n",
    "\n",
    "from sklearn.preprocessing import Binarizer\n",
    "Binarizer(threshold=3).fit_transform(iris.data) # 二值化, 阈值设置为3，返回值为二值化后的数据\n",
    "# 若是特征值大于阈值(threshold)则将特征值赋值为1, 否则为0\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "OneHotEncoder().fit_transform() # 哑编码, 返回值为哑编码后的数据\n",
    "\n",
    "from numpy import vstack, array, nan\n",
    "from sklearn.preprocessing import Imputer\n",
    "# 由于IRIS数据集没有缺失值, 故对数据集新增一个样本, 4个特征均赋值为NaN，表示数据缺失\n",
    "# 使用preproccessing库的Imputer类对数据进行缺失值计算\n",
    "Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data))) # 返回值为计算缺失值后的数据\n",
    "# 参数missing_value为缺失值的表示形式，默认为NaN\n",
    "# 参数strategy为缺失值填充方式, 默认为mean\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "PolynomialFeatures().fit_transform(iris.data)\n",
    "# 参数degree为度, 默认值为2\n",
    "\n",
    "from numpy import log1p # 返回1+x的自然对数\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "# 基于单变元函数的数据变换可以使用一个统一的方式完成\n",
    "# 使用preproccessing库的FunctionTransformer对数据进行对数函数转换\n",
    "FunctionTransformer(log1p).fit_transform(iris.data)\n",
    "# 自定义转换函数为对数函数的数据变换\n",
    "# 第一个参数是单变元函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征选择\n",
    "\t# 方差选择法\n",
    "\t# 相关系数法(SelectKBest): pearsonr, chi2, minepy\n",
    "\t# 递归消除特征法\n",
    "\t# 基于惩罚项的特征选择法\n",
    "\t# 基于树模型的特征选择法\n",
    "\n",
    "# 使用方差选择法, 先要计算各个特征的方差, 然后根据阈值, 选择方差大于阈值的特征\n",
    "# 使用feature_selection库的VarianceThreshold类来选择特征\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "VarianceThreshold(threshold=3).fit_transform(iris.data) # 返回值为特征选择后的数据\n",
    "# 参数threshold为方差的阈值\n",
    "\n",
    "# 先要计算各个特征对目标值的相关系数以及相关系数的P值\n",
    "# 用feature_selection库的SelectKBest类结合相关系数来选择特征\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from scipy.stats import pearsonr\n",
    "SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)\n",
    "# 选择K个最好的特征, 返回选择特征后的数据\n",
    "# 第一个参数为计算评估特征是否好的函数, \n",
    "\t# 该函数输入特征矩阵和目标向量, 输出二元组(评分, P值)的数组\n",
    "\t# 数组第i项为第i个特征的评分和P值\n",
    "# 参数k为选择的特征个数\n",
    "\n",
    "# 经典的卡方检验是检验定性自变量对定性因变量的相关性\n",
    "from sklearn.feature_selection import chi2\n",
    "SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)\n",
    "\n",
    "# 经典的互信息也是评价定性自变量对定性因变量的相关性的\n",
    "from minepy import MINE\n",
    "# 由于MINE的设计不是函数式的, 定义mic方法将其为函数式的, 返回一个二元组, 二元组的第2项设置成固定的P值0.5\n",
    "def mic(x, y):\n",
    "    m = MINE()\n",
    "    m.compute_score(x, y)\n",
    "    return (m.mic(), 0.5)\n",
    "SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)\n",
    "\n",
    "# 递归消除特征法使用一个基模型来进行多轮训练, 每轮训练后, 消除若干权值系数的特征, 再基于新的特征集进行下一轮训练\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)\n",
    "# 参数estimator为基模型\n",
    "# 参数n_features_to_select为选择的特征个数\n",
    "\n",
    "# 基于惩罚项的特征选择法: 使用带惩罚项的基模型, 除了筛选出特征外, 同时也进行了降维\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "# 带L1惩罚项的逻辑回归作为基模型的特征选择\n",
    "SelectFromModel(LogisticRegression(penalty=\"l1\", C=0.1)).fit_transform(iris.data, iris.target)\n",
    "\n",
    "# 基于树模型的特征选择法\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# GBDT作为基模型的特征选择\n",
    "SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "94b8ce2ec9a5bb238eaac0e1491a1a39f3b317a5c834f567228f3f456bc35b60"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
