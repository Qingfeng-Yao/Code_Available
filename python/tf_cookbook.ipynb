{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 基本概念\n",
    "    - tf.logging\n",
    "    - tf.summary\n",
    "    - tf.Print\n",
    "    - tf.estimator\n",
    "    - tf.data\n",
    "    - tf.nn\n",
    "    - TFRecord\n",
    "    - tf中的Feature\n",
    "    - sess\n",
    "    - 数据类型 \n",
    "    - 激活函数\n",
    "    - 损失函数\n",
    "    - 优化器\n",
    "    - 指标\n",
    "    - 异常\n",
    "- 变量\n",
    "- 常见张量计算\n",
    "- 常见张量操作\n",
    "- 基本流程\n",
    "- 构建网络\n",
    "    - scope\n",
    "    - 嵌入层及其他网络层\n",
    "    - 其他操作: 归一化\n",
    "- Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tensorflow.compat.v1 as tf基础\n",
    "- tf.app.run(main=None, argv=None): 处理flag解析，然后执行main函数\n",
    "- tf.compat.v1.disable_v2_behavior(): This function can be called at the beginning of the program(before Tensors, Graphs or other structures have been created, and before devices have been initialized. It switches all global behaviors that are different between TensorFlow 1.x and 2.x to behave as intended for 1.x.\n",
    "- tf.gfile.Exists(filename)\n",
    "- tf.compat.v1.Session(target='', graph=None, config=None): 运行tensorflow操作的类\n",
    "    - tf.Session.reset(FLAGS.master)\n",
    "    - sess = tf.Session(FLAGS.master)\n",
    "    - init = tf.global_variables_initializer()\n",
    "    - sess.run(init)\n",
    "    - 开始迭代，每次迭代执行sess.run\n",
    "- tf.keras.Model: 网络需要继承的类\n",
    "    - 两个函数: build和call\n",
    "- tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(labels, logits, axis=None, name=None, dim=None)\n",
    "\n",
    "## tensorflow_probability as tfp基础\n",
    "- tfp.distributions.OneHotCategorical(logits=None, probs=None, dtype=tf.int32): 由一组类的对数概率参数化；参数logits或probs指明最可能的类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本概念"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tensorflow1.0: 静态图\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:...\n"
     ]
    }
   ],
   "source": [
    "# tf.logging 本身就调用了 logging ，会产生两套 handlers，使得每次tf.logging.info(\"...\") 时会将其中内容打印两遍\n",
    "tf.logging.info(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.compat.v1.logging\n",
    "    # set_verbosity(tf.compat.v1.logging.INFO)\n",
    "    # 除了INFO(20)，还有DEBUG(10)，WARN(30)，ERROR(40)和FATAL(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.summary: 能够保存训练过程以及参数分布图并在tensorboard显示\n",
    "    # tf.summary.scalar(tags, values, collections=None, name=None): 用来显示标量信息\n",
    "    # merged = tf.summary.merge_all()\n",
    "    # train_writer = tf.summary.FileWriter(tflog_path, sess.graph)\n",
    "    # train_writer.add_summary(summary, train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.Print(input, data, message=None, summarize=None, name=None)\n",
    "    # 最低要求两个输入，input和data，input是需要打印的变量的名字，data要求是一个list，里面包含要打印的内容\n",
    "    # message是需要输出的错误信息\n",
    "    # summarize是对每个tensor只打印的条目数量，如果是None，对于每个输入tensor只打印3个元素\n",
    "    # name是op的名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "x=tf.constant([2,3,4,5])\n",
    "x=tf.Print(x,[x,x.shape,'test', x],message='Debug message:',summarize=2)\n",
    "with tf.Session() as sess:\n",
    "    print(x.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.estimator\n",
    "    # 如果使用Estimator，就不用再使用Session\n",
    "    # tf.estimator.RunConfig: 用于控制内部和checkpoints等\n",
    "        # model_dir: 指定存储模型参数，graph等的路径\n",
    "        # save_summary_steps: 每隔多少step就存一次Summaries\n",
    "        # save_checkpoints_steps:每隔多少个step就存一次checkpoint\n",
    "\n",
    "        # save_checkpoints_secs: 每隔多少秒就存一次checkpoint，不可以和save_checkpoints_steps同时指定。如果二者都不指定，则使用默认值，即每600秒存一次。如果二者都设置为None，则不存checkpoints\n",
    "        # keep_checkpoint_max：指定最多保留多少个checkpoints，也就是说当超出指定数量后会将旧的checkpoint删除。当设置为None或0时，则保留所有checkpoints\n",
    "        # log_step_count_steps:该参数的作用是,(相对于总的step数而言)指定每隔多少step就记录一次训练过程中loss的值，同时也会记录global steps/s，通过这个也可以得到模型训练的速度快慢\n",
    "    # tf.estimator.Estimator(model_fn, config, params, model_dir)，\n",
    "        # model_fn为自定义的网络模型函数\n",
    "        # config用于控制内部和checkpoints等\n",
    "        # params该参数的值会传递给model_fn\n",
    "        \n",
    "        # model_dir指定checkpoints和其他日志存放的路径\n",
    "        # estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
    "        # estimator.evaluate(input_fn=test_input_fn, steps=None)\n",
    "        # estimator.predict(predict_input_fn)\n",
    "        # 定义在miniconda3/envs/ctrtf/lib/python3.6/site-packages/tensorflow_estimator/python/estimator下的estimator.py文件中\n",
    "    # tf.estimator.experimental.stop_if_no_decrease_hook:\n",
    "        # 定义early_stopping\n",
    "        # 参数:\n",
    "            # estimator: 模型\n",
    "            # metric_name: 需要追踪的评估指标名称\n",
    "            # max_steps_without_increase: 指定评估指标没有提升的情况下最大运行步数\n",
    "    # tf.estimator.TrainSpec(input_fn, max_steps=None, hooks=None)\n",
    "        # input_fn: 用来指定数据输入\n",
    "        # max_steps: 用来指定训练的最大步数\n",
    "        # hooks: 用来在 session 运行的时候做一些额外的操作\n",
    "        # 定义在miniconda3/envs/ctrtf/lib/python3.6/site-packages/tensorflow_estimator/python/estimator下的training.py文件中\n",
    "    # tf.estimator.EvalSpec(input_fn, steps, throttle_secs)\n",
    "        # steps: 用来指定评估的迭代步数，如果为None，则在整个数据集上评估\n",
    "        # throttle_secs: 多少秒后又开始评估，如果没有新的 checkpoints 产生，则不评估，所以这个间隔是最小值\n",
    "        # 定义在miniconda3/envs/ctrtf/lib/python3.6/site-packages/tensorflow_estimator/python/estimator下的training.py文件中\n",
    "    # tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)\n",
    "        # 定义在miniconda3/envs/ctrtf/lib/python3.6/site-packages/tensorflow_estimator/python/estimator下的training.py文件中\n",
    "    # tf.estimator.EstimatorSpec\n",
    "        # mode: ModeKeys\n",
    "        # predictions\n",
    "        # loss: 训练损失Tensor\n",
    "        # train_op\n",
    "        # eval_metric_ops\n",
    "        # 不同模式需要传入不同参数，对于mode == ModeKeys.TRAIN:必填字段是loss和train_op；对于mode == ModeKeys.EVAL:必填字段是loss；对于mode == ModeKeys.PREDICT:必填字段是predictions\n",
    "    # tf.estimator.ModeKeys.TRAIN\n",
    "    # tf.estimator.ModeKeys.EVAL\n",
    "    # tf.estimator.ModeKeys.PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data\n",
    "    # tf.data.TextLineDataset\n",
    "        # 可处理csv和libsvm文件\n",
    "    # tf.data.TFRecordDataset\n",
    "        # 可处理tfrecord文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.nn\n",
    "    # tf.nn.bias_add\n",
    "    # logits = tf.nn.bias_add(logits, output_bias)\n",
    "    # log_probs = tf.nn.log_softmax(logits, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.python_io.TFRecordWriter(path)-->tf.io.TFRecordWriter\n",
    "    # 为什么使用TFRecord?: TFRecord格式的文件存储形式会很合理的帮我们存储数据。TFRecord内部使用了“Protocol Buffer”二进制数据编码方案，它只占用一个内存块，只需要一次性加载一个二进制文件的方式即可，简单，快速，尤其对大型训练数据很友好。而且当我们的训练数据量比较大的时候，可以将数据分成多个TFRecord文件，来提高处理效率\n",
    "    # writer.write(example.SerializeToString())，其中example.SerializeToString()是将example中的map压缩为二进制文件，更好的节省空间\n",
    "    # example的构建: tf.train.Example(features = None)+tf.train.Features(feature = None)，其中feature是个map，key是要保存数据的名字，value是要保存的数据，格式必须符合tf.train.Feature实例要求\n",
    "    # writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.FixedLenFeature和tf.VarLenFeature-->tf.io.FixedLenFeature和tf.io.VarLenFeature\n",
    "    # 前者返回一个定长的tensor，后者返回不定长的；前者可传入shape参数，如为[]则要求tensor的shape=(batch_size,)，若为[k]，则输出tensor的shape=(batch_size,k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.feature_column和tfrecord搭配使用，特征名要一致\n",
    "# tf.feature_column.categorical_column_with_identity(): \n",
    "    # 参数有特征名、num_buckets和default_value\n",
    "    # 得到one-hot编码，即输出为N*num_buckets的矩阵\n",
    "# tf.feature_column.embedding_column\n",
    "    # 参数有categorical_column、dimension\n",
    "    # 将sparse/categrical特征转化为dense向量\n",
    "# tf.feature_column.numeric_column\n",
    "    # 参数有特征名\n",
    "    # 用于抽取数值类型的特征，即dense特征\n",
    "# tf.feature_column.input_layer(features, feature_columns)\n",
    "    # config = tf.ConfigProto();session = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sess\n",
    "# sess.run()这个里面只能放operation和tensor，还有参数feed_dict\n",
    "# tf.ConfigProto()主要的作用是配置tf.Session的运算方式，比如gpu运算或者cpu运算\n",
    "    # 参数如device_count={'GPU':0, 'CPU':4}\n",
    "# _, loss, cos_loss, output, summary = sess.run([train_op, loss, cos_loss, img_last_layer, merged], feed_dict={img: images, img_label: labels})\n",
    "# sess.close()\n",
    "\n",
    "import keras.backend.tensorflow_backend as KTF\n",
    "# config = tf.ConfigProto()  \n",
    "# config.gpu_options.allow_growth=True / config.allow_soft_placement = True \n",
    "# session = tf.Session(config=config)\n",
    "# KTF.set_session(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "x = tf.constant(1)\n",
    "sess = tf.InteractiveSession()\n",
    "print(x.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype\n",
    "    # tf.int64\n",
    "    # tf.float32\n",
    "    # tf.bool\n",
    "# tf.to_float\n",
    "# tf.to_int32\n",
    "# tf.cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 激活函数\n",
    "    # tf.nn.softmax\n",
    "    # tf.nn.sigmoid\n",
    "    # tf.nn.relu\n",
    "    # tf.nn.sotfplus\n",
    "\n",
    "    # tf.sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "    # tf.nn.sigmoid_cross_entropy_with_logits() # logits的输出维度可为1\n",
    "    # tf.nn.softmax_cross_entropy_with_logits() # 其中labels要求是onehot，logits的输出维度为类别数\n",
    "\n",
    "# tf.keras.backend.binary_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 优化器\n",
    "    # tf.train.AdagradOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指标\n",
    "    # tf.metrics.accuracy\n",
    "        # accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n",
    "    # tf.metrics.auc\n",
    "    # tf.metrics.recall\n",
    "    # tf.metrics.precision\n",
    "    # tf.metrics.true_positives\n",
    "    # tf.metrics.true_negatives\n",
    "    # tf.metrics.false_positives\n",
    "    # tf.metrics.false_negatives\n",
    "    # tf.contrib.metrics.f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 异常\n",
    "    # InvalidArgumentError: 当操作接收到无效参数时触发.例如,如果某个操作接收到的输入张量具有无效的值或形状,则可能发生该情况."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 变量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.constant(list) | tf.constant(list, shape=[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Const_2:0\", shape=(3,), dtype=int32)\n",
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([1,2,3])\n",
    "print(a)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.range()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.identity(input_tensor)：返回一个和input_tensor形状、内容都相同的tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "b = tf.identity(a)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.ones_like | tf.zeros_like\n",
    "# tf.zeros() # 其中shape参数可用tf.shape指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 4 6]]\n",
      "[[3 2]\n",
      " [6 4]]\n",
      "[[2 1]\n",
      " [2 0]]\n",
      "[[1 2]\n",
      " [0 2]]\n",
      "[[0 1]\n",
      " [0 2]\n",
      " [1 0]\n",
      " [1 2]]\n",
      "[[0 1 1]\n",
      " [1 0 1]]\n",
      "[[0 2 3]\n",
      " [4 0 6]]\n",
      "[[8 9 9]\n",
      " [9 8 9]]\n"
     ]
    }
   ],
   "source": [
    "# tf.one_hot(indices, depth, on_value=None, off_value=None) # indices给出一个索引的列表，depth表示类别数，即最后onehot向量的长度\n",
    "    # on_value默认值为1，off_value默认值为0\n",
    "    # one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n",
    "# tf.nn.top_k(input, k) # 这个函数表示要返回input中每行元素最大的值，并返回它们的索引下标\n",
    "a = tf.constant([[1,2,3],[4,4,6]])\n",
    "v, i = tf.nn.top_k(a, 2)\n",
    "values = tf.reshape(v, [-1])\n",
    "output_shape = tf.stack([2,3])\n",
    "idx = tf.constant([[j]*2 for j in range(2)])\n",
    "idx_u = tf.expand_dims(idx, 2)\n",
    "i_u = tf.expand_dims(i, 2)\n",
    "indices = tf.concat([idx_u, i_u], 2)\n",
    "indices = tf.reshape(indices, [-1, indices.get_shape().as_list()[-1]])\n",
    "# c = tf.sparse_to_dense(indices, output_shape, values) # 仍有问题，因为indices的必须按顺序\\\n",
    "indices_sort = tf.sort(i, 1)\n",
    "indices_sort_u = tf.expand_dims(indices_sort, 2)\n",
    "indices_b = tf.concat([idx_u, indices_sort_u], 2)\n",
    "indices_b = tf.reshape(indices_b, [-1, indices_b.get_shape().as_list()[-1]])\n",
    "c = tf.sparse_to_dense(indices_b, output_shape, tf.constant(1))\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(v))\n",
    "    print(sess.run(i))\n",
    "    # print(sess.run(idx))\n",
    "    # print(sess.run(values))\n",
    "    # print(sess.run(indices))\n",
    "    # print(sess.run(c))\n",
    "    print(sess.run(indices_sort))\n",
    "    print(sess.run(indices_b))\n",
    "    print(sess.run(c))\n",
    "    print(sess.run(a*c))\n",
    "    # print(sess.run(tf.ones([2,3]) * (-2 ** 32 + 1)))\n",
    "    print(sess.run(tf.where(tf.cast(c, tf.bool), tf.constant([[9,9,9],[9,9,9]]), tf.constant([[8,8,8],[8,8,8]]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "classes = 3\n",
    "labels = tf.constant([0,1,2])\n",
    "output = tf.one_hot(labels, classes)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.random_normal(shape=[2,3], mean=0, stddev=1) # 默认均值为0，标准差为1\n",
    "    # tf.random.truncated_normal() # 参数同上；若生成的数在某个区间之外，则重新进行生成，保证生成值在均值附近\n",
    "    # shape参数可由tf.shape提供\n",
    "# tf.random_uniform(shape, minval, maxval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不能直接将tf.Tensor作为bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"stack_3:0\", shape=(2,), dtype=int32)\n",
      "Tensor(\"strided_slice_2:0\", shape=(), dtype=int32)\n",
      "[[ 0  0  0 22]\n",
      " [ 0  0  0  0]\n",
      " [ 0  0  0 33]]\n"
     ]
    }
   ],
   "source": [
    "# tf.SparseTensor(values,indices,dense_shape)：三个参数均可使用np.array定义，定义稀疏tensor，可以节省内存\n",
    "# tf.sparse_tensor_to_dense(sp_input,default_value=0)：将 SparseTensor 转换为稠密张量\n",
    "# tf.sparse_to_dense(sparse_indices, output_shape, sparse_values, default_value=0)\n",
    "sparse_indices = tf.constant([[0,3],[2,3]])\n",
    "output_shape = tf.stack([3,4])\n",
    "sparse_values = tf.constant([22,33])\n",
    "a = tf.sparse_to_dense(sparse_indices, output_shape, sparse_values)\n",
    "print(output_shape)\n",
    "with tf.Session() as sess:\n",
    "    print(output_shape[0])\n",
    "    print(sess.run(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.Variable()和tf.get_variable()有不同的创建变量的方式：\n",
    "    # tf.Variable() 每次都会新建变量。如果希望重用（共享）一些变量，就需要用到了get_variable()，它会去搜索变量名，有就直接用，没有再新建\n",
    "# tf.Variable(initial_value=0, name=\"...\", trainable=False, dtype=tf.int64, collections=[...])\n",
    "# tf.get_variable(name=\"...\", shape, collections=[...], initializer=tf.truncated_normal_initializer(stddev=0.001))\n",
    "    # 其中参数shape的值不能为None\n",
    "    # initializer还可取值：tf.zeros_initializer()\n",
    "    # 用tf.float32指明数据类型没用，均是dtype=float32_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.contrib.framework.python.ops import variables as contrib_variables\n",
    "# contrib_variables.model_variable('...', shape, initializer=tf.zeros_initializer(), trainable)\n",
    "\n",
    "# from tensorflow.python.ops import nn_ops\n",
    "# nn_ops.bias_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.get_collection(key)：可以用来获取key集合中的所有变量，返回一个列表。如GraphKeys类包含许多标准集合名，如tf.GraphKeys.REGULARIZATION_LOSSES、tf.GraphKeys.VARIABLES、tf.GraphKeys.TRAINABLE_VARIABLES、tf.GraphKeys.GLOBAL_STEP、tf.GraphKeys.GLOBAL_VARIABLES、tf.GraphKeys.UPDATE_OPS\n",
    "\n",
    "# tf.add_to_collection('all_loss', self.loss)\n",
    "# tf.get_collection('all_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndarray转化为tensor\n",
    "    # 通过 convert_to_tensor 这个函数进行转换"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见张量计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.reduce_sum | tf.reduce_mean | tf.reduce_max\n",
    "    # 如果不指定维度(axis)，则计算所有元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 4 5]\n",
      " [6 7 8]\n",
      " [0 1 2]]\n"
     ]
    }
   ],
   "source": [
    "# tf.unsorted_segment_sum(data, segment_ids, num_segments, name=None)\n",
    "    # data为需要计算的tensor\n",
    "    # segment_ids也是一个tensor，对应元素的值标志着data的下标\n",
    "    # num_segments是一个 tensor,必须是以下类型之一：int32,int64，对应着最后生成结果的维度\n",
    "    # 计算一个张量,使得 output[i] = data[index]，其中index为对应segment_ids中元素值为i的位置 \n",
    "import numpy as np\n",
    "data = tf.constant(np.arange(0,9).reshape(3,3))\n",
    "segment_ids = tf.constant([2,0,1])\n",
    "num_segments = 3\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.unsorted_segment_sum(data, segment_ids, num_segments)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 4 4 4 8 8 8]\n",
      "[ 1  2  3  4  6  7  8  9 11]\n",
      "[0 0 1 2 3 0 4 5 6 7 0 8]\n",
      "[[0 0 1 2]\n",
      " [3 0 4 5]\n",
      " [6 7 0 8]]\n"
     ]
    }
   ],
   "source": [
    "b,k,n=3,3,4\n",
    "indices=tf.constant([[1,2,3],[0,2,3],[0,1,3]])\n",
    "values=tf.constant([[0,1,2],[3,4,5],[6,7,8]]) # 原始矩阵[[-1,0,1,2],[3,2,4,5],[6,7,2,8]]\n",
    "indices_flat = tf.reshape(indices, [-1]) + tf.div(tf.range(b * k), k) * n\n",
    "ret_flat = tf.unsorted_segment_sum(tf.reshape(values, [-1]), indices_flat, b * n)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.div(tf.range(b * k), k) * n))\n",
    "    print(sess.run(indices_flat))\n",
    "    print(sess.run(ret_flat))\n",
    "    print(sess.run(tf.reshape(ret_flat, [b, n])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.pow | tf.square | tf.add | tf.sqrt | tf.exp\n",
    "# tf.squared_difference(x,y): 计算张量 x、y 对应元素差平方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乘法\n",
    "    # tf.multiply(x, y)：两个矩阵中对应元素各自相乘\n",
    "    # tf.matmul(a, b)：将矩阵a乘以矩阵b\n",
    "        # logits = tf.matmul(output_layer, output_weights, transpose_b=True) 其中output_layer最后的维度是hidden_size，output_weights的shape是[num_labels, hidden_size]\n",
    "    # tf.tensordot(a, b, axes)\n",
    "        # 在axes=2轴上相乘指的是将a的最后两个维度与b的前两个维度矩阵相乘，然后将结果累加求和，消除(收缩)这四个维度，矩阵a，b剩下的维度concat，就是所求矩阵维度\n",
    "        # 在axes=[[1,3],[0,2]]上进行tensor相乘，指的是将a的第一个维度、第三个维度concat的维度与b的第0(维度下标从0开始)个维度、第二个维度concat的维度进行矩阵相乘，然后将结果累加求和，消除(收缩)这四个维度，矩阵a，b剩下的维度concat，就是所求矩阵维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a的shape: (2, 1, 3, 2)\n",
      "b的shape: (2, 3, 1)\n",
      "res1 shape: (2, 1, 1)\n",
      "res2 shape: (2, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([0,1,2,1,3,4,5,2,3,4,5,0],shape=[2,1,3,2])\n",
    "b =tf.constant([1,3,2,3,1,2],shape=[2,3,1])\n",
    "res1 = tf.tensordot(a,b,axes=2)\n",
    "res2 = tf.tensordot(a,b,axes=[[1,3],[0,2]])\n",
    "print(\"a的shape:\",a.shape)\n",
    "print(\"b的shape:\",b.shape)\n",
    "print(\"res1 shape:\",res1.shape)\n",
    "print(\"res2 shape:\",res2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.logical_or(x, y)：期望输入布尔类型。输入类型为张量，如果张量包含多个元素，则将按元素进行逻辑或运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.3102722  -0.9942122  -1.876304  ]\n",
      " [-0.16043976 -0.2800879   0.8441572 ]]\n",
      "[array([ 0.57491624, -0.63715005, -0.5160734 ], dtype=float32), array([0.5407484 , 0.12749338, 1.8502275 ], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# 计算均值和方差\n",
    "img = tf.Variable(tf.random_normal([2, 3]))\n",
    "mean, variance = tf.nn.moments(img, axes=0)\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(img))\n",
    "    print(sess.run([mean, variance]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3\n",
      "[3 6 9]\n"
     ]
    }
   ],
   "source": [
    "# tf.math.accumulate_n\n",
    "a=tf.constant(-1)\n",
    "b=tf.constant([1,2,3])\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(tf.math.accumulate_n([a,a,a])))\n",
    "    print(sess.run(tf.math.accumulate_n([b,b,b])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.math.reduce_prod\n",
    "# tf.math.reduce_sum\n",
    "# tf.math.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 常见张量操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor.get_shape().as_list()\n",
    "    # 若获取的对应值是None，如batch_size，一般采用tf.shape(tensor)[]来获取(可在reshape函数中使用)；但在tf.compat.v1.get_variable中的shape参数不接受张量\n",
    "# tf.shape：获取张量的形状；返回的是一个tensor\n",
    "# tensorflow打印张量shape，如遇自定义变量(batch size)则会显示None\n",
    "# tf.reshape(tensor, shape)：若shape为[]，输出标量；若shape为[-1]，则展平张量\n",
    "# tf.expand_dims(input,axis=None)：在axis轴处给input增加一个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 该函数可以返回一个tensor的形状,即使我们的tensor定义时某一维的形状定义为None\n",
    "# def get_shape(tensor):\n",
    "#     static_shape = tensor.shape.as_list()\n",
    "#     dynamic_shape = tf.unstack(tf.shape(tensor)) # 其实加不加tf.unstack都行，结果都是tensor\n",
    "#     dims = [s[1] if s[0] is None else s[0]\n",
    "#             for s in zip(static_shape, dynamic_shape)]\n",
    "#     return dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01855624  0.9438359  -0.8169631 ]\n",
      " [ 0.5846827  -0.23804206 -1.5526854 ]]\n",
      "[[-0.15797293]\n",
      " [-1.8465679 ]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.random_normal(shape=[2,3])\n",
    "b = a[:, 2]\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(a))\n",
    "    print(sess.run(tf.expand_dims(b, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.concat(values, axis)：按照指定的已经存在的轴进行拼接\n",
    "# tf.stack(values, axis)：按照指定的新建的轴进行拼接\n",
    "    # tf.stack() 替代 tf.pack()\n",
    "# tf.unstack(num=None, axis=0)：矩阵分解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.tile(input, multiples)：两个参数形状需要一致，第二个参数指明每个维度的复制次数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.split(input_tensor,num_or_size_splits,axis=0)：num_or_size_splits如果是个整数n，就将输入的tensor分为n个子tensor。如果是个list T，就将输入的tensor分为len(T)个子tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[array([[-0.3797491 ,  0.39872998, -0.23381083]], dtype=float32), array([[-1.3213638 , -0.13312116, -1.4646379 ]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "c = tf.random_normal([2,3])\n",
    "cs = tf.split(c, 2, axis=0)\n",
    "print(type(cs))\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(cs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.argmax | tf.maximum | tf.squeeze\n",
    "\t# tf.argmax(log_probs, axis=-1, output_type=tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.equal(A, B)：对比这两个矩阵或者向量的相等的元素，如果是相等的那就返回True，否则返回False，返回的值的矩阵维度和A是一样的\n",
    "# tf.greater(x, y)\n",
    "# tf.less(x, y)\n",
    "# tf.not_equal(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.where(condition, x=None, y=None)：\n",
    "# 若只有参数condition，则返回condition中为True的索引\n",
    "# 若有x和y，则x, y相同维度，返回值是对应元素，condition中元素为True的元素替换为x中的元素，为False的元素替换为y中对应元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.sequence_mask(lengths,maxlen=None,dtype=tf.bool)：用于数据填充。返回值mask张量：默认mask张量就是布尔格式的一种张量表达，只有True和 False 格式，也可以通过参数dtype指定其他数据格式。参数lengths：表示的是长度；可以是标量，也可以是列表 [ ] ，也可以是二维列表[ [ ],[ ] ,…]，甚至是多维列表…，一般列表类型的用的比较多。参数maxlen：当默认None，默认从lengths中获取最大的那个数字，决定返回mask张量的长度；当为N时，返回的是N长度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'SequenceMask/Less:0' shape=(3, 5) dtype=bool>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.sequence_mask(tf.constant([2,3,4]), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.assign(A, new_number)：这个函数的功能主要是把A的值变为new_number\n",
    "# tf.assign_add(ref,value)：更新ref的值，通过增加value，即：ref = ref + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.cond()类似于if...else..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 10 20 30 40 50 60 70 80 90]\n",
      "[1 1 1 1 1 1 1 1 1 1]\n",
      "[ 1 11 21 31 41 51 61 71 81 91]\n",
      "[11 51 91]\n"
     ]
    }
   ],
   "source": [
    "# tf.gather()用来取出tensor中指定索引位置的元素\n",
    "temp1 = tf.range(0,10)*10\n",
    "temp2 = tf.constant(1, shape=[10])\n",
    "temp3 = temp1+temp2\n",
    "temp4 = tf.gather(temp3, [1,5,9])\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(sess.run(temp1))\n",
    "    print(sess.run(temp2))\n",
    "    print(sess.run(temp3))\n",
    "    print(sess.run(temp4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ScatterNd_5:0\", shape=(8,), dtype=int32)\n",
      "Tensor(\"Cast_1:0\", shape=(8,), dtype=float32)\n",
      "[ 0. 11.  0. 10.  9.  0.  0. 12.]\n"
     ]
    }
   ],
   "source": [
    "# tf.scatter_nd对零张量进行赋值\n",
    "# updates的形状：indices.shape[:−1]+shape[indices.shape[−1]:]=(4,)+(,)=(4,)\n",
    "indices = tf.constant([[4], [3], [1], [7]])\n",
    "updates = tf.constant([9, 10, 11, 12])\n",
    "shape = tf.constant([8])\n",
    "scatter = tf.scatter_nd(indices, updates, shape)\n",
    "print(scatter)\n",
    "scatter = tf.cast(scatter, tf.float32)\n",
    "print(scatter)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(scatter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [5]]\n"
     ]
    }
   ],
   "source": [
    "# tf.slice 抽取数据\n",
    "    # begin表示从inputs的哪几个维度上的哪个元素开始抽取\n",
    "    # size表示在inputs的各个维度上抽取的元素个数\n",
    "    # 若begin[]或size[]中出现-1,表示抽取对应维度上的所有元素\n",
    "x = [[1,2,3],[4,5,6]]\n",
    "with tf.Session() as sess:\n",
    "    begin = [0,1] # 从x[0,1]，即元素2开始抽取\n",
    "    size = [2,1] # 从x[0,1]开始，对x的第一个维度(行)抽取2个元素，对x的第二个维度(行)抽取1个元素\n",
    "    print(sess.run(tf.slice(x, begin, size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.control_dependencies()函数是用来控制计算流图的，也就是给图中的某些计算指定顺序\n",
    "    # 有的时候我们想要指定某些操作执行的依赖关系，比如想要让参数先更新，然后再获取参数更新后的值等\n",
    "    # 如tf.control_dependencies([a,b]): 按先后顺序执行a、b种操作\n",
    "# tf.assign_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b = tf.Variable(tf.zeros([100])) # 100维向量\n",
    "# W = tf.Variable(tf.random_uniform([784, 100], -1, 1))\n",
    "# x = tf.placeholder(dtype=tf.float32, name=\"x\")  / img = tf.placeholder(tf.float32, [None, 256, 256, 3])\n",
    "# relu = tf.nn.relu(tf.matmul(W, x)+b)\n",
    "# C = [...] # Cost\n",
    "# s = tf.Session()\n",
    "# for step in range(0, 10):\n",
    "#     input = 100维向量\n",
    "#     result = s.run(C, feed_dict={x:input})\n",
    "#     print(step, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow变量作用域\n",
    "    # tf.name_scope()、tf.variable_scope()是两个作用域函数，一般与两个创建/调用变量的函数tf.Variable() 和tf.get_variable()搭配使用。常用于：1）变量共享；2）tensorboard画流程图进行可视化封装变量\n",
    "    # tf.name_scope()/命名域、tf.variable_scope()/变量域会在模型中开辟各自的空间，而其中的变量均在这个空间内进行管理\n",
    "        # 命名域还可由tf.op_scope创建，变量域还可由tf.variable_op_scope创建\n",
    "        # 对于上述两种作用域，对于使用tf.Variable()方式创建的变量，具有相同的效果，都会在变量名称前面，加上域名称。对于通过tf.get_variable()方式创建的变量，只有variable scope名称会加到变量名称前面，而name scope不会作为前缀\n",
    "    # variable_scope可以通过设置reuse标志以及初始化方式来影响域下的变量，因为想要达到变量共享的效果, 就要在 tf.variable_scope()的作用域下使用 tf.get_variable() 这种方式产生和提取变量. 不像 tf.Variable() 每次都会产生新的变量, tf.get_variable() 如果遇到了已经存在名字的变量时, 它会单纯的提取这个同样名字的变量，如果不存在名字的变量再创建\n",
    "    # 必须要在tf.variable_scope的作用域下使用tf.get_variable()函数。这里用tf.get_variable( ) 而不用tf.Variable( )，是因为前者拥有一个变量检查机制，会检测已经存在的变量是否设置为共享变量，如果已经存在的变量没有设置为共享变量，TensorFlow 运行到第二个拥有相同名字的变量的时候，就会报错\n",
    "    # with tf.variable_scope(name_or_scope=\"...\", reuse=tf.AUTO_REUSE) as scope:\n",
    "    # with tf.name_scope(\"...\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var1:0\n",
      "my_name_scope/var2:0\n",
      "my_name_scope/Add:0\n"
     ]
    }
   ],
   "source": [
    "with tf.name_scope(\"my_name_scope\"):\n",
    "    v1 = tf.get_variable(\"var1\", [1], dtype=tf.float32) \n",
    "    v2 = tf.Variable(1, name=\"var2\", dtype=tf.float32)\n",
    "    a = tf.add(v1, v2)\n",
    "    print(v1.name)\n",
    "    print(v2.name) \n",
    "    print(a.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my_variable_scope/var1:0\n",
      "my_variable_scope/var2:0\n",
      "my_variable_scope/Add:0\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"my_variable_scope\"):\n",
    "    v1 = tf.get_variable(\"var1\", [1], dtype=tf.float32)\n",
    "    v2 = tf.Variable(1, name=\"var2\", dtype=tf.float32)\n",
    "    a = tf.add(v1, v2)\n",
    "    print(v1.name) \n",
    "    print(v2.name)\n",
    "    print(a.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import variable_scope\n",
    "# with variable_scope.variable_scope(\"name\",reuse=tf.AUTO_REUSE):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.contrib.framework.python.ops import arg_scope\n",
    "# with arg_scope(): 指定网络的默认参数；第一个参数如[layers.conv2d]表示要执行操作的网络，后续的参数就是要设定默认的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 嵌入\n",
    "# tf.nn.embedding_lookup(嵌入矩阵, 索引列表) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.contrib import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers.input_from_feature_columns(columns_to_tensors,feature_columns,scope=tf.variable_scope)\n",
    "# layers.fully_connected(输入, 输出维度, activation_fn=tf.nn.softmax, variables_collections, outputs_collections, scope)\n",
    "# layers.linear(输入, 输出维度, scope, variables_collections, outputs_collections)\n",
    "# layers.batch_norm(input_vec, scale=True, is_training, scope, variables_collections, outputs_collections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.layers.dense\n",
    "# tf.layers.batch_normalization\n",
    "# tf.layers.dropout\n",
    "\n",
    "# output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化\n",
    "# tf.nn.l2_normalize(张量)：元素除以各自的范数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.python.keras import backend\n",
    "# from tensorflow.python.keras.initializers import RandomNormal, Zeros, glorot_normal\n",
    "# from tensorflow.python.keras.layers import Input, Lambda, Embedding, Layer, Flatten, Dropout, Dense\n",
    "    # Input中参数shape不包括batch size，例如shape=(32,) 表示了预期的输入将是一批32维的向量\n",
    "        # Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "    # Layer\n",
    "        # bulid()\n",
    "        # call()\n",
    "    # Embedding(len(word_dict), 300, trainable=True)\n",
    "    # Flatten()(tensor)\n",
    "    # Activation('softmax')(tensor)\n",
    "    # Dot((1, 1))(tensor)\n",
    "    # TimeDistributed，如原始形状为(32, 10, 16)，输出后为(32, 10, 8)，32为batch_size，10为时间步\n",
    "# from tensorflow.python.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.layers.Layer\n",
    "# tf.keras.layers.Concatenate\n",
    "# tf.keras.layers.add\n",
    "# tf.keras.layers.Dense 第一个参数是输出维度\n",
    "# tf.keras.models.Model(inputs, outputs)\n",
    "    # 首先定义好网络，再将网络的输入和输出部分作为参数定义一个Model类对象\n",
    "    # model.compile()\n",
    "        # model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['acc'])\n",
    "    # model.fit()\n",
    "        # verbose: 日志显示，0为不在标准输出流输出日志信息，1为输出进度条记录，2为每个epoch输出一行记录\n",
    "        # model.fit_generator(train_data_batch, epochs=1, steps_per_epoch=len(train_label)//30)\n",
    "    # model.predict()\n",
    "# tf.keras.initializers.glorot_normal\n",
    "# tf.keras.initializers.RandomUniform(-gamma / 100, gamma / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 命名空间\n",
    "\t# 遇到问题: UnicodeDecodeError: 'utf-8' codec can't decode byte 0xb6 in position 2437: invalid start byte\n",
    "\t# 原因在于名字重复，发生碰撞；尽量为新建变量以及网络层(尤其涉及dense层的网络)赋予特定的名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras\n",
    "\t# from keras.layers import * --> class Attention(Layer):  需要有build(self, input_shape)函数，定义相关权重，如self.WQ = self.add_weight(name='WQ', shape=(input_shape[0][-1], self.output_dim), initializer='glorot_uniform', trainable=True)；还需要有call(self, x)函数\n",
    "\t# from keras.models import Model\n",
    "\t# from keras import backend as K\n",
    "\t\t# mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "\t\t# mask = 1 - K.cumsum(mask, 1) # 沿着张量的某一维累积和\n",
    "\t\t# K.expand_dims(mask, 2)\n",
    "\t\t# K.dot(Q_seq, self.WQ)\n",
    "\t\t# K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "\t\t# K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "\t\t# K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "\t\t# K.softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import keras.backend.tensorflow_backend as KTF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.utils import Sequence：定义generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.keras.utils.get_file(\n",
    "      fname=\"aclImdb.tar.gz\", \n",
    "      origin=\"http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\", \n",
    "      extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "with tf.gfile.GFile(os.path.join(directory, file_path), \"r\") as f:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "with tf.Graph().as_default():\n",
    "\tbert_module = hub.Module(BERT_MODEL_HUB)\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        with tf.Session() as sess:\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                            tokenization_info[\"do_lower_case\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "bert_module = hub.Module(BERT_MODEL_HUB, trainable=True)\n",
    "bert_inputs = dict(\n",
    "input_ids=input_ids,\n",
    "input_mask=input_mask,\n",
    "segment_ids=segment_ids)\n",
    "bert_outputs = bert_module(\n",
    "inputs=bert_inputs,\n",
    "signature=\"tokens\",\n",
    "as_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bert\n",
    "tokenizer = bert.tokenization.FullTokenizer(vocab_file=vocab_file, do_lower_case=do_lower_case)\n",
    "tokenizer.tokenize(\"This here's an example of using the BERT tokenizer\")\n",
    "\n",
    "train_op = bert.optimization.create_optimizer(loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n",
    "\n",
    "bert.run_classifier.InputExample\n",
    "bert.run_classifier.convert_examples_to_features\n",
    "bert.run_classifier.input_fn_builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.placeholder_with_default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.unstack 对矩阵进行分解的函数\n",
    "# value：代表需要分解的矩阵变量\n",
    "# axis：指明对矩阵的哪个维度进行分解\n",
    "# num\n",
    "tf.stack 对矩阵进行拼接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random_crop # 随机裁剪函数\n",
    "tf.image.random_flip_left_right # 随机左右翻转图片\n",
    "tf.image.flip_left_right\n",
    "tf.image.crop_to_bounding_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.conv2d(input, filter, strides, padding)\n",
    "# input: 形如[batch, in_height, in_width, in_channels]\n",
    "# filter: 卷积核 形如[filter_height, filter_width, in_channels, out_channels]\n",
    "# strides: 卷积时在图像每一维的步长，这是一个一维的向量，长度4\n",
    "# padding: string类型的量，只能是\"SAME\",\"VALID\"其中之一，这个值决定了不同的卷积方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.max_pool(value, ksize, strides, padding, name=None)\n",
    "# value: 形如[batch, height, width, channels]\n",
    "# ksize: 池化窗口的大小，取一个四维向量，一般是[1, height, width, 1]，因为我们不想在batch和channels上做池化，所以这两个维度设为了1\n",
    "# strides: 和卷积类似，窗口在每一个维度上滑动的步长，一般也是[1, stride,stride, 1]\n",
    "# padding: 和卷积类似，可以取'VALID' 或者'SAME'\n",
    "# 返回形如[batch, height, width, channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.nn.local_response_normalization(input, depth_radius=None, bias=None, alpha=None, beta=None, name=None)\n",
    "# 正则\n",
    "# input: 形如[batch, height, width, channels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = int(np.prod(pool5.get_shape()[1:]))\n",
    "pool5_flat = tf.reshape(pool5, [-1, shape])\n",
    "\n",
    "tf.reshape(tf.reduce_sum(t, 1), [tf.shape(t)[0], 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.transpose\n",
    "tf.clip_by_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.abs\n",
    "tf.boolean_mask\n",
    "tf.subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.exponential_decay\n",
    "# 指数衰减法设置学习率\n",
    "# global_step=tf.Variable(0)\n",
    "# decay_rate为衰减系数\n",
    "# decay_steps为衰减速度\n",
    "# staircase(默认值为False)选择不同的衰减方式：如果staircase = True，那就表明每decay_steps次计算学习速率变化，更新原始学习速率，如果是False，那就是每一步都更新学习速率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.GradientDescentOptimizer(lr).minimize(....., global_step=global_step)\n",
    "\n",
    "opt = tf.train.MomentumOptimizer(learning_rate=lr, momentum=0.9)\n",
    "grads_and_vars = opt.compute_gradients(loss, train_layers) # train_layers为网络层参数列表\n",
    "opt.apply_gradients([(grads_and_vars[0][0], train_layers[0]), ...], global_step=global_step)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5121dbc7a5fd939b11e545b8c787b185a36dfa9d128bb939b33e3da03e1b4a22"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('ctrtf': conda)",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
