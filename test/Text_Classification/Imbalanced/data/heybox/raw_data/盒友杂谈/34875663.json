AI本质也是复读机？阿里和浙大联合推出读唇模型，中英双语实时复述
2006年世界杯上，法国球员齐达内到底被对手的话激怒，狠狠地拿头撞击对方胸口被逐出赛场，他的对手到底说了什么呢？——AI也许可以给出答案。 随着人工智能系统被赋予越来越多的功能，它将帮助我们解决更多问题。学会读唇语的AI暴露了其复读机的本质，可以解读许多被消音的谜团。而AI读唇的真正目的，是成为听力障碍患者的耳朵，帮助他们“听到”原本消失在耳朵里的声音。据统计，目前全世界约有4.66亿的人不幸患有听力障碍，大约占到了世界人口的5%。根据世界卫生组织的预估，到2050年，这个数字将会攀升至9亿。大多数患有听力障碍的人与正常交流会存在困难，手语是其中的一种交流方式。如何帮助听力障碍患者和聋哑人与外界交流，是很多人都在关注的问题。交流首先要解决的是“听懂”别人的话，手语和唇语都可以可以作为一种相互理解的方法。听障患者通过读唇语得以获取信息，但是这个技能需要长时间的练习，并且即使掌握了识别率也很低。将“读唇术”交给AI实际上已经不是什么新鲜事了，早在2016年，谷歌deepmind和牛津大学的研究人员就开发了名为Lipreading视频标注系统，准确率达95.2%，远远优于受训的读唇者。2018年哥伦比亚大学研究团队开发了大型视觉语音识别系统。为了追求更高性能的系统，近日，阿里巴巴、浙江大学和斯蒂文斯理工学院的研究人员共同开发出名为LIBS（Lip by Speech）的算法，在将从语音识别中提取出来的特征当作补充材料，其准确率达业界领先水平，同时还针对目前最大的普通话唇语语料库CMLR进行了训练，读懂中文也不在话下。人工智能如何学会读唇？机器读唇很困难，因为它需要从视频中提取时空特征（位置和运动都很重要)。现大多数机器学习系统只能进行单词分类，而不进行句子级的序列预测。LIBS的研究者表示，这个系统在两个基准上管理着行业领先的准确性，在字符错误率上分别比基准高出7.66%和2.75%，能帮助有一定听力障碍的人群观看无字幕的视频。LIBS可以从视频中的多个层次提取有用的音频信息，包括在序列层、语境层和帧数层。将提取的数据与视频数据对齐，识别对应部分（由于样本数量和缺失值不同，可能存在开头或结尾部分的视频和音频序列长度不一致的情况），并采用了特定的筛选方法对有用特征数据进行了提纯。利用提出的多粒度知识精馏的不同层次，实现视频帧与预测字符之间的对齐（其中纵轴表示视频帧，横轴表示预测字符） LIBS的语音识别和唇语识别都是基于注意力机制的“序列到序列”（Sequence-to-sequence）模型，这是一项机器翻译方法，将序列（例如音频或视频序列）输入转化为标签和注意值的输出。值得注意的是，序列到序列模型在自动语音识别(ASR)领域越来越受欢迎，因为它将传统ASR系统的独立组件折叠成了单一的神经网络。总的来说，LIBS通过引入一种新的过滤策略来从语音识别器中提取特征，并通过采用基于交叉模式对齐的方法，来进行帧级知识提取，从而解决两个序列之间采样率不一致的问题，以实现准确唇语识别。研究人员分别使用LRS2和CMLR数据集在上述模型中训练，LRS2数据集中包含45,000条来自BBC的句子音频，而CMLR，来源于中国网络电视网，是包含100,000条以上自然语言句子的最庞大的普通话唇语语料库（包括3,000以上个的汉字和20,000以上条的词组）。“LIBS减少了对非关键帧的注意力。”一名研究人员在其论文中这样描述，“帧数层知识的提纯进一步加强了对视频帧数特征的分辨能力，能使注意力机制更加集中。”LIBS通过红色区域分析唇语句子越长，读得越准确LIBS的研究团队发现，模型在使用过短句子（如LRS2数据集）进行预训练时得到的结果不大理想，因为解码器从少于14个字母的句子中提取有效信息的难度较大。然而，一旦模型使用最大长度为16个单词的句子进行预训练，解码器由于获得了语境层的知识，对LRS2数据集的句末解码质量有了显著提高。早在1982年，就有Easton和Basala的研究表明，人的唇读能力会随着长单词的出现而增强，这表明了在模糊的沟通渠道中，上下文间的特征词句捕捉时间重要性。2016年的LipNet模型也是针对长句在读唇上的优势构建的。该模式利用时空卷积、递归网络和连接主义时间分类损失，将一个可变长度的视频帧序列映射到文本。LipNet系统通过6个不同的电视节目、共超过10万个句子进行5000小时的训练。最终这个AI系统通过只看每个说话人的嘴唇，就能准确地破译整个短语。读唇AI将成为人类的“复读机”近年来，随着深度学习的发展和训练大数据的可用性，人工智能系统学习读唇取得了前所未有的进步，表现也有了很大的提高。读唇对于人类来说也是一项困难的工作，当人们看到说话人的嘴巴时，通常会被细微的、容易混淆的唇形变化所困惑。所以说，AI读唇的实现具有巨大的实用潜力，它可以用于改善助听器、公共空间中的默写、嘈杂环境中的语音识别、生物识别和无声电影处理。AI读唇不久后将可作为应用程序整合到手机中，这使得听力障碍患者无论走到哪里都能随身携带一只“耳朵”。这样的系统还可以帮助那些因为声带受损而不能说话的人“发出声音”。另外，LIBS团队表示在未来的工作中，他们期待着将同样的框架应用到其他模态中，比如语音和手语系统。本文来源：大数据文摘微信公众号 作者：LYLM、陈若朦如有侵权请联系删除
